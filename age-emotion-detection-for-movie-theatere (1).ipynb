{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10327234,"sourceType":"datasetVersion","datasetId":6394401}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom tensorflow.keras.models import load_model\n\n# Loading  pre-trained models\nage_net = cv2.dnn.readNetFromCaffe(\n    \"/kaggle/input/age-and-emotion-setup/deploy_age.prototxt\",  # Download the required prototxt and model files\n    \"/kaggle/input/age-and-emotion-setup/age_net.caffemodel\"\n)\nemotion_model = load_model(\"/kaggle/input/age-and-emotion-setup/model_a1.json\")  # Replace with your custom emotion model\n\n# Age categories\nAGE_BUCKETS = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(13-19)\", \"(20-29)\", \"(30-39)\", \"(40-50)\", \"(60+)\"]\n\n# Emotion categories\nEMOTION_LABELS = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n\n# Initializimng CSV file for logging\ncsv_file = \"movie_theatre_log.csv\"\ndf = pd.DataFrame(columns=[\"Age\", \"Emotion\", \"Entry Time\"])\ndf.to_csv(csv_file, index=False)\n\n# Processing frame for age detection\ndef detect_age_and_emotion(frame, face):\n    # Extract face region\n    (x, y, w, h) = face\n    face_roi = frame[y:y+h, x:x+w]\n    blob = cv2.dnn.blobFromImage(face_roi, 1.0, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n\n    # Predicting age\n    age_net.setInput(blob)\n    age_preds = age_net.forward()\n    age_index = age_preds[0].argmax()\n    age_label = AGE_BUCKETS[age_index]\n\n    # Predicting emotion if age is between 13 and 60\n    emotion_label = None\n    if \"(13-19)\" in age_label or \"(20-29)\" in age_label or \"(30-39)\" in age_label or \"(40-50)\" in age_label:\n        gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n        gray_face = cv2.resize(gray_face, (48, 48)) / 255.0\n        gray_face = np.expand_dims(gray_face, axis=0)\n        gray_face = np.expand_dims(gray_face, axis=-1)\n        emotion_preds = emotion_model.predict(gray_face)\n        emotion_label = EMOTION_LABELS[np.argmax(emotion_preds)]\n\n    return age_label, emotion_label\n\ndef log_data(age, emotion):\n    entry_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    data = {\"Age\": age, \"Emotion\": emotion, \"Entry Time\": entry_time}\n    df = pd.DataFrame([data])\n    df.to_csv(csv_file, mode=\"a\", header=False, index=False)\n\n# Initializing webcam\ncap = cv2.VideoCapture(0)\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Converting to grayscale and detect faces\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n\n    for face in faces:\n        (x, y, w, h) = face\n        age_label, emotion_label = detect_age_and_emotion(frame, face)\n\n        # Determining the message and rectangle color\n        if \"(0-2)\" in age_label or \"(4-6)\" in age_label or \"(8-12)\" in age_label or \"(60+)\" in age_label:\n            message = \"Not Allowed\"\n            color = (0, 0, 255)  # Red\n        else:\n            message = f\"Emotion: {emotion_label}\"\n            color = (0, 255, 0)  # Green\n\n        # Drawing rectangle and display message\n        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n        cv2.putText(frame, f\"Age: {age_label}\", (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n        cv2.putText(frame, message, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n        # Log data\n        log_data(age_label, emotion_label if emotion_label else \"N/A\")\n\n    # Display the video feed\n    cv2.imshow(\"Age and Emotion Detection\", frame)\n\n    # Break loop on 'q' key press\n    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T15:28:33.812076Z","iopub.execute_input":"2024-12-29T15:28:33.812494Z","iopub.status.idle":"2024-12-29T15:28:53.941402Z","shell.execute_reply.started":"2024-12-29T15:28:33.812464Z","shell.execute_reply":"2024-12-29T15:28:53.939754Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a902218b09d3>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"/kaggle/input/age-and-emotion-setup/age_net.caffemodel\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0memotion_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/age-and-emotion-setup/model_a1.json\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your custom emotion model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Age categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;34mf\"File format not supported: filepath={filepath}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;34m\"Keras 3 only supports V3 `.keras` files and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=/kaggle/input/age-and-emotion-setup/model_a1.json. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(/kaggle/input/age-and-emotion-setup/model_a1.json, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."],"ename":"ValueError","evalue":"File format not supported: filepath=/kaggle/input/age-and-emotion-setup/model_a1.json. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(/kaggle/input/age-and-emotion-setup/model_a1.json, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}